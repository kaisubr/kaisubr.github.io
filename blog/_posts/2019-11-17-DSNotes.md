---
layout: post
category: blog
title: Data Science (overview)
---

## Data Science ~ basics

Here, I'll discuss machine learning on tabular data using tools such as `pandas` and Sci-Kit. 

I realize it's a long article, so if you want, you can skip directly to the applications. I'll apply this information to [predict Rotten Tomatoes ratings for movies](https://github.com/kaisubr/rotten-tomatoes-prediction). 

### [1] Making a decision tree regression model

Pandas is one of the more common data manipulation tools. Install the package as necessary (i.e. `sudo pip install pandas`) and import it into your notebook, i.e. `import pandas as pd`.

* Read a CSV dataset and store it as a DataFrame: `df = pd.read_csv(CSV_FILE_PATH)`
* Summarize data: `df.describe()`
* View the first few rows (entries): `df.head()`

A `DataFrame `organizes data with several columns, and each entry is a row. 

* Display the columns: `df.columns`
* If there are missing values in certain rows, [`dropna(...)`]( https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html ) can drop them: `df = df.dropna(axis=0)`
  * for `axis=0`, the rows with N/A will be dropped.
  * for `axis=1`, the entire column will be dropped if there is a single N/A.
  * See also: `fillna(...)` and `isna(...)`

The column that we are trying to predict, such as predicting house prices, is the **prediction target** (`y`). The other columns are **features** (`X`) which are used to predict the target column.

* Individual columns can be selected as 

  * ```python
    y = df.Price
    ```

* Multiple columns can be selected as

  * ```python
    features = ['Rooms', 'Landsize']
    X = df[features] 
    ```

* Again, `X` and `y` are type DataFrame, so they can be summarized: `X.describe()` and `X.head()`.

Sci-Kit Learn can be used to analyze data organized within a DataFrame. For this ML model, we can use a decision tree (DecisionTreeRegressor) through `from sklearn.tree import DecisionTreeRegressor`. Next, you would **{1}** <u>fit the model</u> using the entries, then **{2}** <u>predict from some new features</u>.

When we fit the model, we need to know the entries' feature columns (many x's) and the respective target (a single y). 

```python
from sklearn.tree import DecisionTreeRegressor

tree_model = DecisionTreeRegressor(random_state=1)
tree_model.fit(X, y)

# Let's predict using the first few houses
predictions = tree_model.predict(X.head())
print(   predictions   )
# And compare it with the actual Price
print(   y.head()   )

```

We can also validate the model we created, to determine how good it is. One such way is through **Mean Absolute Error**, MAE; where absolute error is $e = |\text{actual - predicted}|$, andâ€‹ for $n$ data entries,
$$
\text{MAE} = \frac{ \sum_{i=0}^{n}{e[i]} }{n}
$$
So,

```python
from sklearn.metrics import mean_absolute_error

print(   mean_absolute_error(y, predictions)   )
```

To best analyze the model, we will want to <u>randomly split</u> the given dataset into the training and testing portions. 

```python
from sklearn.model_selection import train_test_split

trainX, testX, trainy, testy = train_test_split(X, y, random_state = 0)
```

That means we can analyze the dataset properly:

```python
tree_model = DecisionTreeRegressor(random_state=1)
tree_model.fit(trainX, trainy)

predictions = tree_model.predict(testX)
print(predictions)

# Print the MAE between the actual target and the predicted y
print(mean_absolute_error(testy, predictions))
```

#### Summary

In summary, we perform the following steps.

1. Remove/impute missing values. 
2. Determine the features (X) and the single prediction target (y), and select them (save them) as type `DataFrame `.
3. Split the dataset's X features and y prediction using `train_test_split` from `sklearn.model_selection`. Drop or apply encoding to non-numerical categorical columns (discussed later).
4. Select a model and set the random state as true.
5. Fit the model using the train X and train y. Note that Kaggle uses notation `X_train`, `X_valid`, `y_train`, `y_valid`.
6. Generate predictions using the test X.
7. Determine the error between the predictions and the test y (actual/expected results). This is the validation (testing) stage.



### [2] Reducing error for our model

When we fit data to the training set, 

* Overfitting occurs if the error is very small when the model is run with the training, but performs extremely poorly with new data (think of a graphical regression of an absurdly high degree)
* Underfitting occurs when the error is large, and the regression is too generic because it does not capture clear patterns (think of a straight line, while the data is a parabolic or cubic, etc). It performs poorly with both the training and new data, so it could be argued as even worse than overfitting.

#### Changing depth of tree

For regression trees, you can change the depth of the tree to see if it affects the MAE, by adding the `max_leaf_nodes=...` argument:

```python
max_leaf_nodes = 50
tree_model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
```

Then we can try it with different values of `max_leaf_nodes`. (Note that `train_and_return_mae( ... )` performs the code snippets outlined above.)

```python
for max_leaf_nodes in [5, 50, 500, 5000]:
    mae = train_and_return_mae(max_leaf_nodes, trainX, testX, trainy, testy)
    print("Max leaf nodes: %d  \t\t Mean Absolute Error:  %d" %(max_leaf_nodes, mae))
```

#### Changing type of model

##### Random forests

You can also try a random forest model (multiple trees) `RandomForestRegressor` to try and reduce error. Simply replace DecisionTreeRegressor with RandomForestRegressor from Sci-Kit.

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# Train and print MAE
forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(trainX, trainy)
predictions = forest_model.predict(testX)

print(   mean_absolute_error(testy, predictions)   )
```

##### XGBoost

Extreme gradient boosting will be discussed in a later section.



## Data Science ~ other techniques

### Alternatives to dropping rows

Suppose the data looked like this.

| Rooms | Landsize | Price |
| ----- | -------- | ----- |
| 3     | 25       | 40    |
| N/A   | N/A      | 50    |
| 2     | 35       | 50    |
| N/A   | 40       | 70    |

Previously we used `dropna` to quickly remove rows with any N/A (missing) values. This is effective since it guarantees that our data is built on real measurements. 

* **Removal** ("Amputation"): Another simple option is to remove columns directly. In the example, the entire Rooms column is removed.

  * ```python
    # Get names of columns (i.e. Rooms, Landsize) that have missing values
    cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]
    
    # Drop them.
    lesstrainX = trainX.drop(cols_with_missing, axis=1)
    lesstestX = testX.drop(cols_with_missing, axis=1)
    
    # Train with model, print MAE.
    # ...
    print(   mean_absolute_error(testy, predictions)   )
    ```

* **Imputation**: We can fill in all N/A values with the average of that column. For example, the N/A above are filled with 2.5 since the average of the existing (not N/A) Rooms column is 2.5.

  * We use SimpleImputer as follows.

  * ```python
    from sklearn.impute import SimpleImputer
    
    # Imputation
    my_imputer = SimpleImputer()
    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))
    
    # Imputation removed column names; put them back
    imputed_X_train.columns = X_train.columns
    imputed_X_valid.columns = X_valid.columns
    
    # Train model using imputed_X_train, imputed_X_valid, y_train, y_valid, and print MAE.
    ```

* **Imputation tracking**: While performing imputation, it is a good idea to keep track of which rows we had changed (create new columns `COL_NAME_was_missing` with boolean values, `true` if N/A had existed in that row).

  * Thus it would look like:

    | Rooms | Landsize | Price | Rooms_was_missing | Landsize_was_missing |
    | ----- | -------- | ----- | ----------------- | -------------------- |
    | 3     | 25       | 40    | false             | false                |
    | 2.5   | 33.33    | 50    | true              | true                 |
    | 2     | 35       | 50    | false             | false                |
    | 2.5   | 40       | 70    | true              | false                |

  * https://www.kaggle.com/alexisbcook/missing-values#Score-from-Approach-3-(An-Extension-to-Imputation):

  * ```python
    # Make copy to avoid changing original data (when imputing)
    X_train_plus = X_train.copy()
    X_valid_plus = X_valid.copy()
    
    # Make new columns indicating what will be imputed
    for col in cols_with_missing:
        X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()
        X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()
    
    # Imputation
    my_imputer = SimpleImputer()
    imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))
    imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))
    
    # Imputation removed column names; put them back
    imputed_X_train_plus.columns = X_train_plus.columns
    imputed_X_valid_plus.columns = X_valid_plus.columns
    
    print("MAE from Approach 3 (An Extension to Imputation):")
    print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))
    ```

### Non-numerical data

Oftentimes data is presented in formats such as dates (November 29, 2019), ratings (TV-MA, PG-13, G, etc.), places (London, New York), and so on. A **categorical variable** is a format that takes a limited number of variables. 

How can we convert non-numerical categorical variables into numerical data?

* **Removal**: Simply remove any columns that contain non-numerical formats. Obviously we lose a lot of data, but it is easy for starting out.

  * Obtain the columns that contain non-numerical data (of type `object`):

    ```python
    s = (X_train.dtypes == 'object')
    object_cols = list(s[s].index)
    
    print(object_cols) #will output ['Date', 'Movie_Ratings', 'Place']
    ```

* **Label encoding**: Translate data into numerical values. Suppose movie ratings are limited to ['G', 'PG', 'R']. Encode them to index values, G = 0, PG = 1, and R = 2.

  * To view all unique variables within a column:

    ```python
    unique = df['Movie_Ratings'].unique()
    print(unique)
    
    # If the data is sortable,
    unique.sort()
    print(unique) # or more simply print(sorted(unique))
    ```

  * To encode the variables within every non-numerical column, use [`Label_Encoder`](https://www.kaggle.com/alexisbcook/categorical-variables#Score-from-Approach-2-(Label-Encoding)):

    ```python
    from sklearn.preprocessing import LabelEncoder
    
    # Make copy to avoid changing original data 
    label_X_train = X_train.copy()
    label_X_valid = X_valid.copy()
    
    # Apply label encoder to each column with categorical data
    label_encoder = LabelEncoder()
    for col in object_cols:
        label_X_train[col] = label_encoder.fit_transform(X_train[col])
        label_X_valid[col] = label_encoder.transform(X_valid[col])
    
    print("MAE from Approach 2 (Label Encoding):") 
    print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))
    
    ```

* **One-hot encoding**: Translate data 

* into the presence of data. Suppose the column Movie_Ratings are limited to G, PG, and R. Create 3 new columns: G, PG, and R; then fill them with 1 if that row is G/PG/R respectively, and 0 if not. Typically this is used when there isn't a type of "ranking" associated with variables (such as color: Red isn't necessarily greater than Blue, so label encoding might not make sense). For the sake of consistency, we can keep looking at movie ratings:

  * | Movie_Ratings | G    | PG   | R    |
    | ------------- | ---- | ---- | ---- |
    | G             | 1    | 0    | 0    |
    | G             | 1    | 0    | 0    |
    | PG            | 0    | 1    | 0    |

  * Use `OneHotEncoder` as follows (https://www.kaggle.com/alexisbcook/categorical-variables#Score-from-Approach-3-(One-Hot-Encoding)):

  * ```python
    from sklearn.preprocessing import OneHotEncoder
    
    # Apply one-hot encoder to each column with categorical data
    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))
    
    # One-hot encoding removed index; put it back
    OH_cols_train.index = X_train.index
    OH_cols_valid.index = X_valid.index
    
    # Remove categorical columns (will replace with one-hot encoding)
    num_X_train = X_train.drop(object_cols, axis=1)
    num_X_valid = X_valid.drop(object_cols, axis=1)
    
    # Add one-hot encoded columns to numerical features
    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
    
    print("MAE from Approach 3 (One-Hot Encoding):") 
    print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))
    ```

  * These three columns become features used in the model.



### Ensemble methods

An **ensemble method** combines multiple models. In the case of random forests, we used multiple decision trees to reduce the MAE. 

#### Gradient boosting

**Gradient boosting** is a method that iteratively adds models into an ensemble. 

* First, the current ensemble (a single model) is used to predict and the loss is calculated (i.e. MAE). 
  * Then, the gradient descent on the loss is used to train a new model as parameters that would reduce the current loss. 
* This new model is added to the ensemble, and the process is repeated.

The extreme gradient boosting library provides a regressor model that does this for you: 

```python
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error

xg_model = XGBRegressor()
xg_model.fit(trainX, trainy)

pred = xg_model.predict(testX)
print("MAE = " + str(mean_absolute_error(pred, testy)))
```

The regressor model has parameters that can be tuned:

* `n_estimators`: the number of iterations (i.e. the number of models in the ensemble). Values range from 100-1000, and too high can cause overfitting while too low causes underfitting.
* `learning_rate`: causes each tree added to the ensemble to help less, therefore a smaller rate and a larger amount of estimators provides more accurate models, although it will take a longer time to train. Values are about 0.1.
* ` early_stopping_rounds`: finds the time to stop iterations. Values are about 5-10: the model will stop after 5 consecutive rounds of decreasing validation scores.
* `n_jobs`: parallel threads can help compute faster, so `n_jobs` is equivalent to the number of cores in your processor. Typically this is 2-4.

Here is a sample model: 

```python
xg_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=2)

xg_model.fit(trainX, trainy, 
             early_stopping_rounds=5, 
             eval_set=[(testX, testy)], 
             verbose=False)
```